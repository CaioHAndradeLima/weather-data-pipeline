#!/bin/bash
set -e
echo "Setting up local environment for Retail Data Pipeline"
echo ""

ENV_FILE=".env"

# Helper function to ask for input
ask() {
  local var_name=$1
  local prompt=$2
  local secret=${3:-false}

  if [ "$secret" = true ]; then
    read -s -p "$prompt: " value
    echo ""
  else
    read -p "$prompt: " value
  fi

  if [ -z "$value" ]; then
    echo "âŒ $var_name cannot be empty"
    exit 1
  fi

  echo "$var_name=$value" >> "$ENV_FILE"
  export "$var_name=$value"
}

# Backup existing .env
if [ -f "$ENV_FILE" ]; then
  cp "$ENV_FILE" "$ENV_FILE.bak.$(date +%s)"
  echo "ðŸ“¦ Existing .env backed up"
fi

# Start fresh
echo "# Auto-generated by setup.sh" > "$ENV_FILE"

echo "Snowflake configuration"
echo ""
echo "You can query in your Snowflake web the following SQL"
echo "SELECT CURRENT_ACCOUNT(), CURRENT_USER(), CURRENT_REGION(), CURRENT_ROLE();"
echo ""
ask "SNOWFLAKE_ACCOUNT"   "Snowflake account (example: YS80657.us-east-2.aws)"
ask "SNOWFLAKE_USER"      "Snowflake user"
ask "SNOWFLAKE_PASSWORD"  "Snowflake password" true
ask "SNOWFLAKE_ROLE"      "Snowflake role"
ask "SNOWFLAKE_WAREHOUSE" "Snowflake warehouse (e.g. RETAIL_WH)"
ask "SNOWFLAKE_DATABASE"  "Snowflake database (e.g. RETAIL_ANALYTICS)"
ask "SNOWFLAKE_SCHEMA"    "Snowflake schema (e.g. BRONZE)"

echo ""
echo "Setting up your Airflow / Postgres configuration"
ask "POSTGRES_USER"     "Postgres user" false
ask "POSTGRES_PASSWORD" "Postgres password" true
ask "POSTGRES_DB"       "Postgres database"

echo ""
echo "Airflow security"
ask "AIRFLOW_FERNET_KEY" "Airflow Fernet key (generate if you don't have one)"

echo ""
PROJECT_ROOT_DIRECTORY="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
echo "PROJECT_ROOT=$PROJECT_ROOT_DIRECTORY" >> .env


# Postgres retail (CDC + incremental reads)
echo "RETAIL_PG_HOST=local-postgres" >> .env
echo "RETAIL_PG_PORT=5432" >> .env
echo "RETAIL_PG_DB=retail_prod" >> .env
echo "RETAIL_PG_USER=retail_user" >> .env
echo "RETAIL_PG_PASSWORD=retail_password" >> .env

echo ""
echo ".env file successfully created!"

echo ""
echo "Setting up dbt profile and snowflake dbt"
python -m pip install dbt-core dbt-snowflake

DBT_DIR="dbt"
DBT_PROFILES_FILE="$DBT_DIR/profiles.yml"

# Create ~/.dbt if it doesn't exist
mkdir -p "$DBT_DIR"

# Backup existing profiles.yml if present
if [ -f "$DBT_PROFILES_FILE" ]; then
  cp "$DBT_PROFILES_FILE" "$DBT_PROFILES_FILE.bak.$(date +%s)"
  echo "Existing dbt profiles.yml backed up"
fi

cat > "$DBT_PROFILES_FILE" <<EOF
retail_pipeline:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: $SNOWFLAKE_ACCOUNT
      user: $SNOWFLAKE_USER
      password: $SNOWFLAKE_PASSWORD
      role: $SNOWFLAKE_ROLE
      warehouse: $SNOWFLAKE_WAREHOUSE
      database: $SNOWFLAKE_DATABASE
      schema: SILVER
      threads: 4
EOF

echo "dbt profile created at ~/.dbt/profiles.yml"
